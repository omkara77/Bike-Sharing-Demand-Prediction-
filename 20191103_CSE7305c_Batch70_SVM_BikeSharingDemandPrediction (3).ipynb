{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Bike Sharing Demand Prediction using SVM </center>\n",
    "\n",
    "___\n",
    "\n",
    "SVM Model objective is to find the maximum margin classifier. The maximum margin classifier helps to reduces the hypothesis space, effect of high dimensionality and computation. \n",
    "\n",
    "The points which maximum margin classifier touches are called support vectors. These vectors alone are enough to classify all other points.\n",
    "\n",
    "<img src=\"svm.png\" alt=\"https://becominghuman.ai/ensemble-learning-bagging-and-boosting-d20f38be9b1e\" width=\"700\" height=\"700\">\n",
    "\n",
    "___\n",
    "\n",
    "## Bike Sharing Demand Data\n",
    "\n",
    "We will use [Bike Sharing Demand Dataset](https://www.kaggle.com/c/bike-sharing-demand/data). We are provided hourly rental data spanning two years. The training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. \n",
    "\n",
    "| Feature | Description |\n",
    "| --- | --- |\n",
    "| datetime | hourly date + timestamp |\n",
    "| season |  1 = spring, 2 = summer, 3 = fall, 4 = winter |\n",
    "| holiday | whether the day is considered a holiday |\n",
    "| workingday | whether the day is neither a weekend nor holiday |\n",
    "| weather | 1 = Clear, Few clouds, Partly cloudy, Partly cloudy |\n",
    "|  | 2 = Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist |\n",
    "|  | 3 = Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds |\n",
    "|  | 4 = Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog |\n",
    "| temp | temperature in Celsius |\n",
    "| atemp | \"feels like\" temperature in Celsius |\n",
    "| humidity | relative humidity |\n",
    "| windspeed | wind speed |\n",
    "| casual | number of non-registered user rentals initiated |\n",
    "| registered | number of registered user rentals initiated |\n",
    "| count | number of total rentals |\n",
    "\n",
    "**The goal is to predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period.**\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T09:51:49.919693Z",
     "start_time": "2019-11-03T09:51:49.914374Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# skip warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T09:51:50.299465Z",
     "start_time": "2019-11-03T09:51:50.255172Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'data/bike-sharing-demand.csv' does not exist: b'data/bike-sharing-demand.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-e936f08f7619>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m bike_sharing = pd.read_csv('data/bike-sharing-demand.csv',\n\u001b[1;32m----> 5\u001b[1;33m                            parse_dates = [\"datetime\"])\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mbike_sharing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'data/bike-sharing-demand.csv' does not exist: b'data/bike-sharing-demand.csv'"
     ]
    }
   ],
   "source": [
    "### parse [\"datetime\"] columns as dates while reading the data.Check parse_dates in pd.read_csv\n",
    "### parsing dateime will read sting of datetime and stores it in datetimeformat.\n",
    "\n",
    "bike_sharing = pd.read_csv('data/bike-sharing-demand.csv',\n",
    "                           parse_dates = [\"datetime\"])\n",
    "\n",
    "bike_sharing.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a total of 10866 observations in the data set and 12 features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Let us take a look at a portion of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T09:51:50.815257Z",
     "start_time": "2019-11-03T09:51:50.799422Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bike_sharing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1919a50f4d81>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbike_sharing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'bike_sharing' is not defined"
     ]
    }
   ],
   "source": [
    "bike_sharing.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check if all columns are in appropriate data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T09:51:51.146683Z",
     "start_time": "2019-11-03T09:51:51.137804Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime      datetime64[ns]\n",
       "season                 int64\n",
       "holiday                int64\n",
       "workingday             int64\n",
       "weather                int64\n",
       "temp                 float64\n",
       "atemp                float64\n",
       "humidity               int64\n",
       "windspeed            float64\n",
       "casual                 int64\n",
       "registered             int64\n",
       "count                  int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike_sharing.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All features have the appropriate data types.\n",
    "\n",
    "We will extract all time features from datetime field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T09:51:51.501456Z",
     "start_time": "2019-11-03T09:51:51.466788Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>season</th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weather</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>count</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Day Of Week</th>\n",
       "      <th>Hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-01 01:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-01 02:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-01 03:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-01 04:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime  season  holiday  workingday  weather  temp   atemp  \\\n",
       "0 2011-01-01 00:00:00       1        0           0        1  9.84  14.395   \n",
       "1 2011-01-01 01:00:00       1        0           0        1  9.02  13.635   \n",
       "2 2011-01-01 02:00:00       1        0           0        1  9.02  13.635   \n",
       "3 2011-01-01 03:00:00       1        0           0        1  9.84  14.395   \n",
       "4 2011-01-01 04:00:00       1        0           0        1  9.84  14.395   \n",
       "\n",
       "   humidity  windspeed  casual  registered  count  Year  Month  Day  \\\n",
       "0        81        0.0       3          13     16  2011      1    1   \n",
       "1        80        0.0       8          32     40  2011      1    1   \n",
       "2        80        0.0       5          27     32  2011      1    1   \n",
       "3        75        0.0       3          10     13  2011      1    1   \n",
       "4        75        0.0       0           1      1  2011      1    1   \n",
       "\n",
       "   Day Of Week  Hour  \n",
       "0            5     0  \n",
       "1            5     1  \n",
       "2            5     2  \n",
       "3            5     3  \n",
       "4            5     4  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Seperate and create new variables for year,month ,Day,Day of week ,Hour from datetime column.\n",
    "\n",
    "bike_sharing['Year'] = bike_sharing['datetime'].dt.year\n",
    "bike_sharing['Month'] = bike_sharing['datetime'].dt.month\n",
    "bike_sharing['Day'] = bike_sharing['datetime'].dt.day\n",
    "bike_sharing['Day Of Week'] = bike_sharing['datetime'].dt.dayofweek\n",
    "bike_sharing['Hour'] = bike_sharing['datetime'].dt.hour\n",
    "\n",
    "bike_sharing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set datetime features as index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T09:51:51.839779Z",
     "start_time": "2019-11-03T09:51:51.831020Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['datetime', 'season', 'holiday', 'workingday', 'weather', 'temp',\n",
       "       'atemp', 'humidity', 'windspeed', 'casual', 'registered', 'count',\n",
       "       'Year', 'Month', 'Day', 'Day Of Week', 'Hour'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike_sharing.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T09:51:52.024968Z",
     "start_time": "2019-11-03T09:51:52.016219Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Set datetime column to index\n",
    "\n",
    "bike_sharing.set_index('datetime', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T09:51:52.215763Z",
     "start_time": "2019-11-03T09:51:52.201239Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp',\n",
       "       'humidity', 'windspeed', 'casual', 'registered', 'count', 'Year',\n",
       "       'Month', 'Day', 'Day Of Week', 'Hour'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike_sharing.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us seperate our numerical and categorical attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T09:51:52.620238Z",
     "start_time": "2019-11-03T09:51:52.612203Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['temp', 'atemp', 'humidity', 'windspeed', 'casual', 'registered',\n",
       "       'Year', 'Month', 'Day', 'Day Of Week', 'Hour'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a list for all categorical column names\n",
    "cat_cols = ['season', 'holiday', 'workingday', 'weather']\n",
    "\n",
    "# create a list for all numerical column names\n",
    "num_cols = bike_sharing.columns[~bike_sharing.columns.isin(cat_cols)]\n",
    "\n",
    "# remove target column('count')\n",
    "num_cols = num_cols.drop('count')\n",
    "\n",
    "num_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will convert all categorical variable to category type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T09:51:53.021351Z",
     "start_time": "2019-11-03T09:51:53.001427Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Apply Type conversion \n",
    "\n",
    "\n",
    "bike_sharing[cat_cols] = bike_sharing[cat_cols].apply(lambda x : x.astype('category'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will check there are no invalid values in any of the categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T09:51:53.455919Z",
     "start_time": "2019-11-03T09:51:53.438267Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "season ----->> [1, 2, 3, 4]\n",
      "Categories (4, int64): [1, 2, 3, 4] \n",
      "\n",
      "holiday ----->> [0, 1]\n",
      "Categories (2, int64): [0, 1] \n",
      "\n",
      "workingday ----->> [0, 1]\n",
      "Categories (2, int64): [0, 1] \n",
      "\n",
      "weather ----->> [1, 2, 3, 4]\n",
      "Categories (4, int64): [1, 2, 3, 4] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Check unique values of each categorical column\n",
    "\n",
    "for x in cat_cols :\n",
    "    print(x, '----->>', bike_sharing[x].unique(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no invalid values.\n",
    "\n",
    "Let us check numerical data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T09:51:54.003938Z",
     "start_time": "2019-11-03T09:51:53.927124Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Day Of Week</th>\n",
       "      <th>Hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10886.00000</td>\n",
       "      <td>10886.000000</td>\n",
       "      <td>10886.000000</td>\n",
       "      <td>10886.000000</td>\n",
       "      <td>10886.000000</td>\n",
       "      <td>10886.000000</td>\n",
       "      <td>10886.000000</td>\n",
       "      <td>10886.000000</td>\n",
       "      <td>10886.000000</td>\n",
       "      <td>10886.000000</td>\n",
       "      <td>10886.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>20.23086</td>\n",
       "      <td>23.655084</td>\n",
       "      <td>61.886460</td>\n",
       "      <td>12.799395</td>\n",
       "      <td>36.021955</td>\n",
       "      <td>155.552177</td>\n",
       "      <td>2011.501929</td>\n",
       "      <td>6.521495</td>\n",
       "      <td>9.992559</td>\n",
       "      <td>3.013963</td>\n",
       "      <td>11.541613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.79159</td>\n",
       "      <td>8.474601</td>\n",
       "      <td>19.245033</td>\n",
       "      <td>8.164537</td>\n",
       "      <td>49.960477</td>\n",
       "      <td>151.039033</td>\n",
       "      <td>0.500019</td>\n",
       "      <td>3.444373</td>\n",
       "      <td>5.476608</td>\n",
       "      <td>2.004585</td>\n",
       "      <td>6.915838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.82000</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2011.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>13.94000</td>\n",
       "      <td>16.665000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>7.001500</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>2011.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>20.50000</td>\n",
       "      <td>24.240000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>12.998000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>2012.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>26.24000</td>\n",
       "      <td>31.060000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>16.997900</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>2012.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>41.00000</td>\n",
       "      <td>45.455000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>56.996900</td>\n",
       "      <td>367.000000</td>\n",
       "      <td>886.000000</td>\n",
       "      <td>2012.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              temp         atemp      humidity     windspeed        casual  \\\n",
       "count  10886.00000  10886.000000  10886.000000  10886.000000  10886.000000   \n",
       "mean      20.23086     23.655084     61.886460     12.799395     36.021955   \n",
       "std        7.79159      8.474601     19.245033      8.164537     49.960477   \n",
       "min        0.82000      0.760000      0.000000      0.000000      0.000000   \n",
       "25%       13.94000     16.665000     47.000000      7.001500      4.000000   \n",
       "50%       20.50000     24.240000     62.000000     12.998000     17.000000   \n",
       "75%       26.24000     31.060000     77.000000     16.997900     49.000000   \n",
       "max       41.00000     45.455000    100.000000     56.996900    367.000000   \n",
       "\n",
       "         registered          Year         Month           Day   Day Of Week  \\\n",
       "count  10886.000000  10886.000000  10886.000000  10886.000000  10886.000000   \n",
       "mean     155.552177   2011.501929      6.521495      9.992559      3.013963   \n",
       "std      151.039033      0.500019      3.444373      5.476608      2.004585   \n",
       "min        0.000000   2011.000000      1.000000      1.000000      0.000000   \n",
       "25%       36.000000   2011.000000      4.000000      5.000000      1.000000   \n",
       "50%      118.000000   2012.000000      7.000000     10.000000      3.000000   \n",
       "75%      222.000000   2012.000000     10.000000     15.000000      5.000000   \n",
       "max      886.000000   2012.000000     12.000000     19.000000      6.000000   \n",
       "\n",
       "               Hour  \n",
       "count  10886.000000  \n",
       "mean      11.541613  \n",
       "std        6.915838  \n",
       "min        0.000000  \n",
       "25%        6.000000  \n",
       "50%       12.000000  \n",
       "75%       18.000000  \n",
       "max       23.000000  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#numeric data\n",
    "\n",
    "bike_sharing[num_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T09:51:54.156975Z",
     "start_time": "2019-11-03T09:51:54.142616Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "season         0\n",
       "holiday        0\n",
       "workingday     0\n",
       "weather        0\n",
       "temp           0\n",
       "atemp          0\n",
       "humidity       0\n",
       "windspeed      0\n",
       "casual         0\n",
       "registered     0\n",
       "count          0\n",
       "Year           0\n",
       "Month          0\n",
       "Day            0\n",
       "Day Of Week    0\n",
       "Hour           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check missing values\n",
    "\n",
    "bike_sharing.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The data looks clean. There are no NA values as count for all is equal to number of rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building\n",
    "\n",
    "___\n",
    "\n",
    "We will seperate our independent variables and target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T09:51:56.253463Z",
     "start_time": "2019-11-03T09:51:56.247436Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# independent variables\n",
    "X = bike_sharing.drop('count', axis = 1)\n",
    "\n",
    "# dependent variable\n",
    "y = bike_sharing['count'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to prepare data\n",
    "\n",
    "We will write a function to prepare data for following functions.\n",
    "\n",
    "- Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T09:51:57.170384Z",
     "start_time": "2019-11-03T09:51:57.162830Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_scaler = StandardScaler()\n",
    "\n",
    "def prepare_data(X, y, split_size = 0.3) :\n",
    "    \n",
    "    \n",
    "    ## train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                        test_size = split_size)  \n",
    "    \n",
    "    ## fit num_scaler\n",
    "    num_scaler.fit(X_train[num_cols])\n",
    "    \n",
    "    print(X_train.shape)\n",
    "    print(X_test.shape)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to pre-process data\n",
    "\n",
    "We will write a function to pre-process data for following functions. This function will be called to transform both train and test datasets.\n",
    "\n",
    "- Scale the numeric features\n",
    "- Dummify the categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T09:51:58.322744Z",
     "start_time": "2019-11-03T09:51:58.317339Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess_data(data, scale = False) :\n",
    "    \n",
    "    # scale numeric features\n",
    "    if scale == True :\n",
    "        #tranform numeric data using num_scaler\n",
    "        data[num_cols] = num_scaler.transform(data[num_cols])\n",
    "    \n",
    "    # dummify categorical features\n",
    "    \n",
    "    data = pd.get_dummies(data, drop_first = False)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for Model Fit & Predict\n",
    "\n",
    "We will write a function for following functions. \n",
    "\n",
    "- Fit the model on train data\n",
    "- Perform cross-validation when needed\n",
    "- Predict on train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T09:51:59.346101Z",
     "start_time": "2019-11-03T09:51:59.338312Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def model_building(X, y, test, model, params = None, k = 1) :\n",
    "    \n",
    "    if params == None :\n",
    "        \n",
    "        ## Fit model \n",
    "        model.fit(X, y)\n",
    "        \n",
    "        # return fitted model & train-test predictions\n",
    "        return (model, model.predict(X), model.predict(test))\n",
    "    \n",
    "    else :\n",
    "        \n",
    "        model_cv = GridSearchCV(model, param_grid = params, cv = k)\n",
    "        \n",
    "        ## Fit model_cv using \n",
    "        model_cv.fit(X, y)\n",
    "        \n",
    "        ## check best estimator \n",
    "        model = model_cv.best_estimator_\n",
    "        \n",
    "        print(model_cv.best_estimator_)\n",
    "        # return and extra object for all cross validation operations\n",
    "        return (model_cv, model, model.predict(X), model.predict(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T09:52:00.248947Z",
     "start_time": "2019-11-03T09:52:00.240360Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def model_evaluation(y_train, pred_train, y_test, pred_test) :\n",
    "    \n",
    "    print('''\n",
    "            =========================================\n",
    "               MAE and MSE FOR TRAIN DATA\n",
    "            =========================================''')\n",
    "    print(\"Mean Absolute Error : \", mean_absolute_error(y_train, pred_train), \n",
    "          \"\\nMean Squared Error : \", mean_squared_error(y_train, pred_train))\n",
    "    \n",
    "    print('''\n",
    "            =========================================\n",
    "               MAE and MSE FOR TEST DATA\n",
    "            =========================================''')\n",
    "    print(\"Mean Absolute Error : \", mean_absolute_error(y_test, pred_test), \n",
    "          \"\\nMean Squared Error : \", mean_squared_error(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building\n",
    "\n",
    "___\n",
    "\n",
    "\n",
    "### Let us build a [Support Vector Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html)\n",
    "\n",
    "Call the train test split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T09:52:01.150911Z",
     "start_time": "2019-11-03T09:52:01.130682Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8708, 15)\n",
      "(2178, 15)\n"
     ]
    }
   ],
   "source": [
    "## Use prepare_data and get X_train, X_test, y_train, y_test where split size = 0.2\n",
    "\n",
    "X_train, X_test, y_train, y_test = prepare_data(X, y, 0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build SVM Model without scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T09:52:22.309948Z",
     "start_time": "2019-11-03T09:52:12.934285Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            =========================================\n",
      "               MAE and MSE FOR TRAIN DATA\n",
      "            =========================================\n",
      "Mean Absolute Error :  136.83523793321186 \n",
      "Mean Squared Error :  34627.014766050495\n",
      "\n",
      "            =========================================\n",
      "               MAE and MSE FOR TEST DATA\n",
      "            =========================================\n",
      "Mean Absolute Error :  137.3219767557702 \n",
      "Mean Squared Error :  35019.47824118046\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "# Call the pre-process function for both train and test data.\n",
    "X_train_nsc = preprocess_data(X_train)\n",
    "X_test_nsc = preprocess_data(X_test)\n",
    "\n",
    "# Call the model building function\n",
    "model, pred_train, pred_test = model_building(X_train_nsc, y_train,\n",
    "                                              X_test_nsc, SVR())\n",
    "\n",
    "# Call the model evaluation function for both train and test data to view model performance\n",
    "model_evaluation(y_train, pred_train, y_test, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build SVM Model with scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T09:52:32.720919Z",
     "start_time": "2019-11-03T09:52:22.312544Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            =========================================\n",
      "               MAE and MSE FOR TRAIN DATA\n",
      "            =========================================\n",
      "Mean Absolute Error :  22.689945361586155 \n",
      "Mean Squared Error :  2116.686429143097\n",
      "\n",
      "            =========================================\n",
      "               MAE and MSE FOR TEST DATA\n",
      "            =========================================\n",
      "Mean Absolute Error :  23.201565896044812 \n",
      "Mean Squared Error :  2204.1848808473987\n"
     ]
    }
   ],
   "source": [
    "# Call the pre-process function for both train and test data.\n",
    "X_train = preprocess_data(X_train,scale= True)\n",
    "X_test = preprocess_data(X_test,scale= True)\n",
    "\n",
    "# Call the model building function\n",
    "model, pred_train, pred_test = model_building(X_train, y_train,\n",
    "                                              X_test, SVR())\n",
    "\n",
    "# Call the model evaluation function for both train and test data to view model performance\n",
    "model_evaluation(y_train, pred_train, y_test, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the model performance improves drastically by standardising the numerical data.\n",
    "\n",
    "Let us tune parameters to build non-linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Tuning in SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C parameter** is a regularization parameter that controls the trade off between the achieving a low training error and a low testing error that is the ability to generalize your classifier to unseen data. \n",
    "\n",
    "The C parameter trades off misclassification of training examples against simplicity of the decision surface. A low C makes the decision surface smooth, while a high C aims at classifying all training examples correctly by giving the model freedom to select more samples as support vectors.\n",
    "\n",
    "\n",
    "\n",
    "**Gamma** is the parameter of a RBF Kernel (to handle non-linear classification).\n",
    "\n",
    "> ð‘˜(ð‘¥ð‘›,ð‘¥ð‘š)=ð‘’ð‘¥ð‘(âˆ’ð›¾||ð‘¥ð‘›âˆ’ð‘¥ð‘š||^2)\n",
    "\n",
    "\n",
    ">gamma =1 / (2*sigma ^2)\n",
    "\n",
    "\n",
    "Intuitively, the gamma parameter defines how far the influence of a single training example reaches, with low values meaning â€˜farâ€™ and high values meaning â€˜closeâ€™. The gamma parameters can be seen as the inverse of the radius of influence of samples selected by the model as support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T09:54:17.244402Z",
     "start_time": "2019-11-03T09:52:32.724113Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR(C=0.1, cache_size=200, coef0=0.0, degree=2, epsilon=0.1,\n",
      "    gamma='auto_deprecated', kernel='poly', max_iter=-1, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "\n",
      "            =========================================\n",
      "               MAE and MSE FOR TRAIN DATA\n",
      "            =========================================\n",
      "Mean Absolute Error :  124.36719389307528 \n",
      "Mean Squared Error :  28670.822565895418\n",
      "\n",
      "            =========================================\n",
      "               MAE and MSE FOR TEST DATA\n",
      "            =========================================\n",
      "Mean Absolute Error :  124.1582591891073 \n",
      "Mean Squared Error :  28811.769726175546\n"
     ]
    }
   ],
   "source": [
    "parameters = {'C' : [0.01,0.1], \n",
    "              'degree' : [2, 3],\n",
    "              'kernel' : ['poly']}\n",
    "\n",
    "# Call the model building function\n",
    "model_cv, model, pred_train, pred_test = model_building(X_train, y_train, X_test, SVR(), parameters, 10)\n",
    "\n",
    "# Call the model evaluation function for both train and test data to view model performance\n",
    "model_evaluation(y_train, pred_train, y_test, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T09:55:31.548714Z",
     "start_time": "2019-11-03T09:54:17.247306Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR(C=0.1, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.01,\n",
      "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
      "\n",
      "            =========================================\n",
      "               MAE and MSE FOR TRAIN DATA\n",
      "            =========================================\n",
      "Mean Absolute Error :  119.22000622928682 \n",
      "Mean Squared Error :  27552.62580351137\n",
      "\n",
      "            =========================================\n",
      "               MAE and MSE FOR TEST DATA\n",
      "            =========================================\n",
      "Mean Absolute Error :  119.1487664152684 \n",
      "Mean Squared Error :  27806.246411278014\n"
     ]
    }
   ],
   "source": [
    "parameters = {'C' : [0.01, 0.1], \n",
    "              'gamma' : [0.01], \n",
    "              'kernel' : ['rbf']}\n",
    "\n",
    "# Call the model building function\n",
    "model_cv, model, pred_train, pred_test = model_building(X_train, y_train, X_test, SVR(), parameters, 10)\n",
    "\n",
    "# Call the model evaluation function for both train and test data to view model performance\n",
    "model_evaluation(y_train, pred_train, y_test, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model performance deteriotes in higher dimension. This is an indication that data is linearly disrtibuted.\n",
    "\n",
    "# Stacking\n",
    "\n",
    "___\n",
    "\n",
    "Let us build some more models and stack their outputs.\n",
    "\n",
    "- Linear Regression\n",
    "- Knn\n",
    "- Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T07:19:57.705637Z",
     "start_time": "2019-11-03T07:19:57.701879Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you have not noticed yet, this dataset has casual users count and registered users count which adds up to give our target feature count.  \n",
    "`count = casual + registered`\n",
    "\n",
    "We can build a simple linear model with only these two features and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T06:21:59.364336Z",
     "start_time": "2019-11-03T06:21:59.316883Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            =========================================\n",
      "               MAE and MSE FOR TRAIN DATA\n",
      "            =========================================\n",
      "Mean Absolute Error :  7.70811985668323e-14 \n",
      "Mean Squared Error :  9.234363949063356e-27\n",
      "\n",
      "            =========================================\n",
      "               MAE and MSE FOR TEST DATA\n",
      "            =========================================\n",
      "Mean Absolute Error :  7.645899014418982e-14 \n",
      "Mean Squared Error :  9.510491770601982e-27\n"
     ]
    }
   ],
   "source": [
    "# linear model\n",
    "X_train_cr = X_train_nsc[['casual', 'registered']]\n",
    "X_test_cr = X_test_nsc[['casual', 'registered']]\n",
    "\n",
    "model_cr, pred_train_cr, pred_test_cr = model_building(X_train_cr, y_train, X_test_cr,\n",
    "                                                       LinearRegression())\n",
    "\n",
    "model_evaluation(y_train, pred_train_cr, y_test, pred_test_cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T06:21:59.390726Z",
     "start_time": "2019-11-03T06:21:59.368283Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we get the coefficient as 1. This means `1*casual + 1*registered = count` which is what we expected.\n",
    "\n",
    "> If the client shares a dataset with such features then you can charge them for not being able to do a simple addition. ðŸ˜Š\n",
    "\n",
    "We will remove one of these features and train various models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T06:21:59.457359Z",
     "start_time": "2019-11-03T06:21:59.395556Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_ncr = X.drop(['casual'], axis=1)\n",
    "\n",
    "X_train_ncr, X_test_ncr, y_train, y_test = train_test_split(X_ncr, y, test_size = 0.4)\n",
    "\n",
    "X_train_ncr = preprocess_data(X_train_ncr)\n",
    "X_test_ncr = preprocess_data(X_test_ncr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T06:21:59.500497Z",
     "start_time": "2019-11-03T06:21:59.460263Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            =========================================\n",
      "               MAE and MSE FOR TRAIN DATA\n",
      "            =========================================\n",
      "Mean Absolute Error :  22.75570581707355 \n",
      "Mean Squared Error :  1101.8313993747606\n",
      "\n",
      "            =========================================\n",
      "               MAE and MSE FOR TEST DATA\n",
      "            =========================================\n",
      "Mean Absolute Error :  22.829412805600967 \n",
      "Mean Squared Error :  1111.808859699993\n"
     ]
    }
   ],
   "source": [
    "# linear model\n",
    "model_lr, pred_train_lr, pred_test_lr = model_building(X_train_ncr, y_train, X_test_ncr,\n",
    "                                                       LinearRegression())\n",
    "\n",
    "model_evaluation(y_train, pred_train_lr, y_test, pred_test_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net\n",
    "\n",
    ">minimize (1 / (2 * n_samples) * ||y - Xw||^2 +\n",
    "            alpha * l1_ratio * ||w||_1 + \n",
    "            0.5 * alpha * (1 - l1_ratio) * ||w||^2\n",
    "\n",
    "\n",
    "\n",
    "ð¿1_ratio is the ratio between ð¿1 and ð¿2 penalty, ranging from 0 (ridge) to 1 (lasso)\n",
    "\n",
    "\n",
    "\n",
    "alpha :\n",
    "Constant that multiplies the penalty terms. Defaults to 1.0. See the notes for the exact mathematical meaning of this parameter.``alpha = 0`` is equivalent to an ordinary least square, solved by the LinearRegression object. For numerical reasons, using alpha = 0 with the Lasso object is not advised. Given this, you should use the LinearRegression object.\n",
    "\n",
    "l1_ratio : \n",
    "The ElasticNet mixing parameter, with 0 <= l1_ratio <= 1. For l1_ratio = 0 the penalty is an L2 penalty. For l1_ratio = 1 it is an L1 penalty. For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T06:22:03.947467Z",
     "start_time": "2019-11-03T06:21:59.504123Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet(alpha=0.2, copy_X=True, fit_intercept=True, l1_ratio=1,\n",
      "           max_iter=1000, normalize=False, positive=False, precompute=False,\n",
      "           random_state=None, selection='cyclic', tol=0.0001, warm_start=False)\n",
      "\n",
      "            =========================================\n",
      "               MAE and MSE FOR TRAIN DATA\n",
      "            =========================================\n",
      "Mean Absolute Error :  22.643131322100164 \n",
      "Mean Squared Error :  1105.6941225579772\n",
      "\n",
      "            =========================================\n",
      "               MAE and MSE FOR TEST DATA\n",
      "            =========================================\n",
      "Mean Absolute Error :  22.734960313844997 \n",
      "Mean Squared Error :  1114.9692578372365\n"
     ]
    }
   ],
   "source": [
    "# ElasticNet model\n",
    "parameters={'alpha' : [0.2, 0.5],'l1_ratio' : [0, 0.5, 1]}\n",
    "\n",
    "model_elr_cv, model_elr, pred_train_elr, pred_test_elr = model_building(X_train_ncr, y_train,\n",
    "                                                                        X_test_ncr, \n",
    "                                                                        ElasticNet(), \n",
    "                                                                        params=parameters,k=10)\n",
    "\n",
    "model_evaluation(y_train, pred_train_elr, \n",
    "                 y_test, pred_test_elr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN model\n",
    "\n",
    "**p** :\n",
    "    - Power parameter for the Minkowski metric. When p = 1, this is\n",
    "      equivalent to using manhattan_distance (l1), and euclidean_distance\n",
    "\n",
    "\n",
    "**weights** :\n",
    "    - 'uniform' : uniform weights.  All points in each neighborhood\n",
    "      are weighted equally.\n",
    "    - 'distance' : weight points by the inverse of their distance.\n",
    "      in this case, closer neighbors of a query point will have a\n",
    "      greater influence than neighbors which are further away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T06:22:07.591600Z",
     "start_time": "2019-11-03T06:22:03.951311Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                    metric_params=None, n_jobs=None, n_neighbors=6, p=1,\n",
      "                    weights='distance')\n",
      "\n",
      "            =========================================\n",
      "               MAE and MSE FOR TRAIN DATA\n",
      "            =========================================\n",
      "Mean Absolute Error :  0.0 \n",
      "Mean Squared Error :  0.0\n",
      "\n",
      "            =========================================\n",
      "               MAE and MSE FOR TEST DATA\n",
      "            =========================================\n",
      "Mean Absolute Error :  14.674469086168044 \n",
      "Mean Squared Error :  649.401932677201\n"
     ]
    }
   ],
   "source": [
    "##model building\n",
    "\n",
    "model_knn_cv, model_knn, pred_train_knn, pred_test_knn = model_building(X_train_ncr, y_train,\n",
    "                                                                        X_test_ncr, \n",
    "                                                                        KNeighborsRegressor(), \n",
    "                                                                        {'n_neighbors' : [4, 5, 6], \n",
    "                                                                        'weights' : ['uniform', \n",
    "                                                                                     'distance'],\n",
    "                                                                        'p' : [1, 2]}, 10)\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "model_evaluation(y_train, pred_train_knn,\n",
    "                 y_test, pred_test_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T07:27:52.590926Z",
     "start_time": "2019-11-03T07:27:51.114732Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeRegressor(criterion='mse', max_depth=8, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "\n",
      "            =========================================\n",
      "               MAE and MSE FOR TRAIN DATA\n",
      "            =========================================\n",
      "Mean Absolute Error :  10.951277759377462 \n",
      "Mean Squared Error :  330.0162954901755\n",
      "\n",
      "            =========================================\n",
      "               MAE and MSE FOR TEST DATA\n",
      "            =========================================\n",
      "Mean Absolute Error :  14.268730637728801 \n",
      "Mean Squared Error :  635.6676442738384\n"
     ]
    }
   ],
   "source": [
    "## Decision Tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "## Model \n",
    "model_dt_cv, model_dt, pred_train_dt, pred_test_dt = model_building(X_train_ncr, y_train,\n",
    "                                                                    X_test_ncr, \n",
    "                                                       DecisionTreeRegressor(),\n",
    "                                                       {'max_depth' : [7, 8, 9, 10, 12]}, 10)\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "model_evaluation(y_train, pred_train_dt, y_test, pred_test_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although Knn and DT seem like they have overfitted because train error is very low compared to test error, yet we can see by using grid search that training a simpler model is increasing the test error. These are the best model for this data.\n",
    "\n",
    "Let us average these classifier prediction to build a stacked model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T06:25:03.763012Z",
     "start_time": "2019-11-03T06:25:03.753437Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error :  15.769511236395369 \n",
      "Mean Squared Error :  589.7942558281143\n"
     ]
    }
   ],
   "source": [
    "pred_test_stack = (pred_test_lr + pred_test_elr + pred_test_knn + pred_test_dt)/4\n",
    "\n",
    "print(\"Mean Absolute Error : \", mean_absolute_error(y_test, pred_test_stack), \n",
    "      \"\\nMean Squared Error : \", mean_squared_error(y_test, pred_test_stack))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the linear models didn't give good result, we will use only Knn and DT predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T06:25:06.495389Z",
     "start_time": "2019-11-03T06:25:06.487126Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error :  12.381835712329611 \n",
      "Mean Squared Error :  451.10851368359425\n"
     ]
    }
   ],
   "source": [
    "pred_test_stack = (pred_test_knn + pred_test_dt)/2\n",
    "\n",
    "print(\"Mean Absolute Error : \", mean_absolute_error(y_test, pred_test_stack), \n",
    "      \"\\nMean Squared Error : \", mean_squared_error(y_test, pred_test_stack))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that the stacked model has considerably reduced the error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T08:58:04.981157Z",
     "start_time": "2019-11-03T08:58:04.968274Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=8, max_features=None,\n",
       "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save to file in the current working directory\n",
    "pkl_filename = \"pickle_model_DT.pkl\"\n",
    "\n",
    "\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(model_dt, file)\n",
    "\n",
    "# Load from file\n",
    "with open(pkl_filename, 'rb') as file:\n",
    "    pickle_model = pickle.load(file)\n",
    "    \n",
    "\n",
    "pickle_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T09:00:32.951047Z",
     "start_time": "2019-11-03T09:00:32.945059Z"
    }
   },
   "outputs": [],
   "source": [
    "test_preds_dt=pickle_model.predict(X_test_ncr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T09:00:40.010982Z",
     "start_time": "2019-11-03T09:00:40.002640Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4355,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds_dt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T09:00:52.732096Z",
     "start_time": "2019-11-03T09:00:52.725338Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4355,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
